{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQB4PB6ju2xU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install faiss-cpu\n",
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxxPsNwQu4wI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAEwu33ahU3d"
   },
   "source": [
    "## Datu sagatavošana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV3Fnl_6vCSQ"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BItC-ASwITq"
   },
   "outputs": [],
   "source": [
    "titles = dataset.unique(\"title\")\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biNtRDi7zQqv"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_SAMPLES = 50\n",
    "K_VALUES = [1, 3, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHWEZ6xwwzB0"
   },
   "outputs": [],
   "source": [
    "by_title = defaultdict(list)\n",
    "for ex in dataset:\n",
    "    by_title[ex[\"title\"]].append(ex)\n",
    "\n",
    "rng = random.Random(SEED)\n",
    "titles = sorted(by_title.keys())\n",
    "selected_titles = rng.sample(titles, N_SAMPLES)\n",
    "\n",
    "eval_samples = []\n",
    "for t in selected_titles:\n",
    "    eval_samples.append(by_title[t][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZkUZZdNx073"
   },
   "outputs": [],
   "source": [
    "with open(\"KD-RAG-eval.json\", \"w\") as f:\n",
    "    json.dump(eval_samples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnJBbF9zzxkM"
   },
   "outputs": [],
   "source": [
    "contexts = [ex[\"context\"] for ex in eval_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Jxa-GNO0kSL"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=80, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    cid = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunks.append({\n",
    "            \"chunk_id\": cid,\n",
    "            \"text\": \" \".join(words[start:end]),\n",
    "            \"word_start\": start,\n",
    "            \"word_end\": end\n",
    "        })\n",
    "        cid += 1\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def char_to_word_span(text, char_start, char_end):\n",
    "    words = text.split()\n",
    "    pos = 0\n",
    "\n",
    "    for i, w in enumerate(words):\n",
    "        w_start = text.find(w, pos)\n",
    "        w_end = w_start + len(w)\n",
    "        pos = w_end\n",
    "\n",
    "        if w_end > char_start:\n",
    "            ws = i\n",
    "            break\n",
    "\n",
    "    for j in range(i, len(words)):\n",
    "        w_start = text.find(words[j], pos)\n",
    "        if w_start >= char_end:\n",
    "            we = j\n",
    "            break\n",
    "        we = j + 1\n",
    "\n",
    "    return ws, we\n",
    "\n",
    "def overlaps(chunk, ans_ws, ans_we):\n",
    "    return not (\n",
    "        ans_we <= chunk[\"word_start\"]\n",
    "        or ans_ws >= chunk[\"word_end\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL3WQl8n2rsO"
   },
   "outputs": [],
   "source": [
    "lengths = np.array([len(c.split()) for c in contexts])\n",
    "print(\"N contexts:\", len(lengths))\n",
    "print(\"mean:\", lengths.mean())\n",
    "print(\"median:\", np.median(lengths))\n",
    "print(\"p75:\", np.percentile(lengths, 75))\n",
    "print(\"p90:\", np.percentile(lengths, 90))\n",
    "print(\"max:\", lengths.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_e94B8m1Sdi"
   },
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "chunks_mapped = []\n",
    "for ctx_id, ctx in enumerate(contexts):\n",
    "    ctx_chunks = chunk_text(ctx)\n",
    "    for ch in ctx_chunks:\n",
    "        all_chunks.append(ch)\n",
    "        chunks_mapped.append({\n",
    "            \"context_id\": ctx_id\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8U-DRR54fSW"
   },
   "outputs": [],
   "source": [
    "len(all_chunks) == len(chunks_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KodMKC5r5Cob"
   },
   "outputs": [],
   "source": [
    "question_data = []\n",
    "\n",
    "for i, ex in enumerate(eval_samples):\n",
    "    question_data.append({\n",
    "        \"question\": ex[\"question\"],\n",
    "        \"answers\": ex[\"answers\"],\n",
    "        \"context_id\": i,\n",
    "        \"context\": ex[\"context\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VWata216jEg"
   },
   "outputs": [],
   "source": [
    "chunks_by_context = defaultdict(list)\n",
    "for i, meta in enumerate(chunks_mapped):\n",
    "    chunks_by_context[meta[\"context_id\"]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbagLtpI6V4y"
   },
   "outputs": [],
   "source": [
    "def char_to_word_span(text, char_start, char_end):\n",
    "    words = text.split()\n",
    "    pos = 0\n",
    "    spans = []\n",
    "\n",
    "    for w in words:\n",
    "        s = text.find(w, pos)\n",
    "        e = s + len(w)\n",
    "        spans.append((s, e))\n",
    "        pos = e\n",
    "\n",
    "    ws = we = None\n",
    "    for i, (s, e) in enumerate(spans):\n",
    "        if ws is None and e > char_start:\n",
    "            ws = i\n",
    "        if s < char_end:\n",
    "            we = i + 1\n",
    "\n",
    "    return ws, we\n",
    "\n",
    "def overlaps(chunk, ans_ws, ans_we):\n",
    "    return not (\n",
    "        ans_we <= chunk[\"word_start\"]\n",
    "        or ans_ws >= chunk[\"word_end\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhdUsAgX60M5"
   },
   "outputs": [],
   "source": [
    "question_to_chunks = {}\n",
    "\n",
    "for q in question_data:\n",
    "    relevant = set()\n",
    "\n",
    "    for ans_text, ans_start in zip(\n",
    "        q[\"answers\"][\"text\"], q[\"answers\"][\"answer_start\"]\n",
    "    ):\n",
    "        ans_end = ans_start + len(ans_text)\n",
    "        ws, we = char_to_word_span(q[\"context\"], ans_start, ans_end)\n",
    "\n",
    "        for chunk_idx in chunks_by_context[q[\"context_id\"]]:\n",
    "            chunk = all_chunks[chunk_idx]\n",
    "            if overlaps(chunk, ws, we):\n",
    "                relevant.add(chunk_idx)\n",
    "\n",
    "    question_to_chunks[q[\"question\"]] = sorted(relevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEGsoOrdhZxU"
   },
   "source": [
    "## Izgūšanas komponentes bāzlīnijas izvērtēšana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7iQdmnYkEUE"
   },
   "source": [
    "#### Leksiskās izgūšanas metodes izvērtēšana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J27Ufhi-hcxm"
   },
   "outputs": [],
   "source": [
    "bm25_corpus = [ch[\"text\"].lower().split() for ch in all_chunks]\n",
    "bm25 = BM25Okapi(bm25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGz5JWhBiOrj"
   },
   "outputs": [],
   "source": [
    "def bm25_retrieve(query, k):\n",
    "    scores = bm25.get_scores(query.lower().split())\n",
    "    topk = np.argsort(scores)[::-1][:k]\n",
    "    return list(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-63S4jgpiPxY"
   },
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved, relevant):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len(set(retrieved) & set(relevant)) / len(relevant)\n",
    "\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len(set(retrieved) & set(relevant)) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ma7E0kDDiWhO"
   },
   "outputs": [],
   "source": [
    "bm25_recall = {k: [] for k in K_VALUES}\n",
    "bm25_precision = {k: [] for k in K_VALUES}\n",
    "\n",
    "for q in question_data:\n",
    "    query = q[\"question\"]\n",
    "    relevant = question_to_chunks[query]\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        retrieved = bm25_retrieve(query, k)\n",
    "        bm25_recall[k].append(recall_at_k(retrieved, relevant))\n",
    "        bm25_precision[k].append(precision_at_k(retrieved, relevant, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWHaLVvclB2H"
   },
   "source": [
    "#### Semantiskās, blīvās izgūšanas bāzlīnijas izvērtēšana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9EFCV3TlIwz"
   },
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmSai2vllOv1"
   },
   "outputs": [],
   "source": [
    "chunk_texts = [ch[\"text\"] for ch in all_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yacl2dTZlTMV"
   },
   "outputs": [],
   "source": [
    "chunk_embeddings = embedder.encode(\n",
    "    chunk_texts,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADUhme_klVXo"
   },
   "outputs": [],
   "source": [
    "print(chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odwBSH1Hlgj6"
   },
   "outputs": [],
   "source": [
    "faiss.normalize_L2(chunk_embeddings)\n",
    "dim = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(chunk_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPX0u3hElt4E"
   },
   "outputs": [],
   "source": [
    "def dense_retrieve(query, k):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = index.search(q_emb, k)\n",
    "    return list(indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCewuzjalvMb"
   },
   "outputs": [],
   "source": [
    "dense_recall = {k: [] for k in K_VALUES}\n",
    "dense_precision = {k: [] for k in K_VALUES}\n",
    "\n",
    "for q in question_data:\n",
    "    query = q[\"question\"]\n",
    "    relevant = question_to_chunks[query]\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        retrieved = dense_retrieve(query, k)\n",
    "        dense_recall[k].append(recall_at_k(retrieved, relevant))\n",
    "        dense_precision[k].append(precision_at_k(retrieved, relevant, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfcjzkJ9l5DO"
   },
   "outputs": [],
   "source": [
    "print(\"\\nBM25\")\n",
    "for k in K_VALUES:\n",
    "    print(\n",
    "        f\"@{k}: Recall={np.mean(bm25_recall[k]):.3f} | \"\n",
    "        f\"Precision={np.mean(bm25_precision[k]):.3f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nDense\")\n",
    "for k in K_VALUES:\n",
    "    print(\n",
    "        f\"@{k}: Recall={np.mean(dense_recall[k]):.3f} | \"\n",
    "        f\"Precision={np.mean(dense_precision[k]):.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVTUIEAcmxQ_"
   },
   "source": [
    "#### Izgūšanas komponentes bāzlīnijas izvērtēšana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEfeCR1znhF3"
   },
   "outputs": [],
   "source": [
    "MODEL = \"google/flan-t5-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "llm = AutoModelForSeq2SeqLM.from_pretrained(MODEL).to(device)\n",
    "llm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXCE5t1ln6gn"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_answer(prompt, max_new_tokens=32):\n",
    "    inputs = tok(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    out = llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=1\n",
    "    )\n",
    "    return tok.decode(out[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVI0p_ICpy2x"
   },
   "outputs": [],
   "source": [
    "def build_prompt_no_rag(question):\n",
    "    return (\n",
    "        \"Answer the question with a short factual phrase.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "def build_prompt_rag(question, retrieved_chunk_texts):\n",
    "    context_block = \"\\n\\n\".join(retrieved_chunk_texts)\n",
    "    return (\n",
    "        \"Use ONLY the context provided below to answer with a short factual phrase.\\n\"\n",
    "        \"If the answer is NOT in the context, say: unknown.\\n\\n\"\n",
    "        f\"Context:\\n{context_block}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWw28U4POkmo"
   },
   "outputs": [],
   "source": [
    "K_RAG = 3\n",
    "results = []\n",
    "\n",
    "for q in question_data:\n",
    "    question = q[\"question\"]\n",
    "    gold_answers = q[\"answers\"][\"text\"]\n",
    "\n",
    "    prompt_no_rag = build_prompt_no_rag(question)\n",
    "    pred_no_rag = generate_answer(prompt_no_rag)\n",
    "\n",
    "    retrieved_ids = dense_retrieve(question, K_RAG)\n",
    "    retrieved_chunks = [all_chunks[i][\"text\"] for i in retrieved_ids]\n",
    "\n",
    "    prompt_rag = build_prompt_rag(question, retrieved_chunks)\n",
    "    pred_rag = generate_answer(prompt_rag)\n",
    "\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"gold_answers\": gold_answers,\n",
    "        \"no_rag_answer\": pred_no_rag,\n",
    "        \"rag_answer\": pred_rag,\n",
    "        \"retrieved_chunks\": retrieved_chunks\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdKaMZpBqaR6"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"rag_eval.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"question\",\n",
    "        \"gold_answers\",\n",
    "        \"no_rag_answer\",\n",
    "        \"rag_answer\"\n",
    "    ])\n",
    "\n",
    "    for r in results:\n",
    "        writer.writerow([\n",
    "            r[\"question\"],\n",
    "            \" | \".join(r[\"gold_answers\"]),\n",
    "            r[\"no_rag_answer\"],\n",
    "            r[\"rag_answer\"]\n",
    "        ])\n",
    "\n",
    "\n",
    "with open(\"rag_eval_full.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApRY6-OjQlKF"
   },
   "source": [
    "Eksportētie dati un atbilstošās metrikas tiek analizētas manuāli"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
